{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3737453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toasty/repos/NLP/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.ru.stop_words import STOP_WORDS\n",
    "from datasets import load_dataset\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afa84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Нормализация текста...\n",
      " Построение биграмм/триграмм...\n",
      "['турция', 'три', 'сторона', 'окружить', 'морями', 'запад', 'эгейским', 'север', 'чёрный', 'средиземный']\n"
     ]
    }
   ],
   "source": [
    "def load_sib200_ru():\n",
    "    trainset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='train')\n",
    "    valset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='validation')\n",
    "    testset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='test')\n",
    "\n",
    "    X_train, y_train = trainset['text'], trainset['category']\n",
    "    X_val, y_val = valset['text'], valset['category']\n",
    "    X_test, y_test = testset['text'], testset['category']\n",
    "\n",
    "    categories = sorted(list(set(y_train)))\n",
    "    y_train = [categories.index(it) for it in y_train]\n",
    "    y_val = [categories.index(it) for it in y_val]\n",
    "    y_test = [categories.index(it) for it in y_test]\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), categories\n",
    "\n",
    "train_data, val_data, test_data, classes_list = load_sib200_ru()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "# заменяем ссылки, почты и числа спецтокенами.\n",
    "def normalize_text(s: str, nlp_pipeline: spacy.Language):\n",
    "    s = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', s)\n",
    "    s = re.sub(r'\\S+@\\S+', '<EMAIL>', s)\n",
    "    s = re.sub(r'\\d+', '<NUM>', s)\n",
    "    doc = nlp_pipeline(s)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space or token.lower_ in STOP_WORDS:\n",
    "            continue\n",
    "        lemma = token.lemma_.lower()\n",
    "        if len(lemma) <= 2:\n",
    "            continue\n",
    "        tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "print(\" Нормализация текста...\")\n",
    "X_train_norm = [normalize_text(it, nlp) for it in train_data[0]]\n",
    "X_val_norm = [normalize_text(it, nlp) for it in val_data[0]]\n",
    "X_test_norm = [normalize_text(it, nlp) for it in test_data[0]]\n",
    "\n",
    "# Биграммы / триграммы\n",
    "\n",
    "print(\" Построение биграмм/триграмм...\")\n",
    "bigram = Phrases(X_train_norm, min_count=5, threshold=10, delimiter='_')\n",
    "trigram = Phrases(bigram[X_train_norm], min_count=5, threshold=10, delimiter='_')\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "def make_ngrams(texts):\n",
    "    return [trigram_mod[bigram_mod[toks]] for toks in texts]\n",
    "\n",
    "X_train_norm = make_ngrams(X_train_norm)\n",
    "X_val_norm = make_ngrams(X_val_norm)\n",
    "X_test_norm = make_ngrams(X_test_norm)\n",
    "\n",
    "print(X_train_norm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f935185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Обучение TF-IDF...\n",
      " Загрузка FastText...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toasty/repos/NLP/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\" Обучение TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "tfidf.fit(X_train_norm)\n",
    "idf_dict = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "print(\" Загрузка FastText...\")\n",
    "MODEL_PATH_VEC = \"./models/cc.ru.300.vec\"\n",
    "w2v = KeyedVectors.load_word2vec_format(MODEL_PATH_VEC, binary=False)\n",
    "\n",
    "def sentence_vector_tfidf(tokens):\n",
    "    vecs, weights = [], []\n",
    "    for w in tokens:\n",
    "        if w in w2v and w in idf_dict:\n",
    "            vecs.append(w2v[w])\n",
    "            weights.append(idf_dict[w])\n",
    "    if not vecs:\n",
    "        return np.zeros(w2v.vector_size)\n",
    "    vecs = np.array(vecs)\n",
    "    weights = np.array(weights).reshape(-1, 1)\n",
    "    return np.sum(vecs * weights, axis=0) / np.sum(weights)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b736eeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Векторизация...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "print(\" Векторизация...\")\n",
    "X_train_vec = np.array([sentence_vector_tfidf(toks) for toks in X_train_norm])\n",
    "X_val_vec = np.array([sentence_vector_tfidf(toks) for toks in X_val_norm])\n",
    "X_test_vec = np.array([sentence_vector_tfidf(toks) for toks in X_test_norm])\n",
    "\n",
    "pca = PCA(n_components=120, random_state=42)\n",
    "X_train_vec = pca.fit_transform(X_train_vec)\n",
    "X_val_vec = pca.transform(X_val_vec)\n",
    "X_test_vec = pca.transform(X_test_vec)\n",
    "\n",
    "\n",
    "X_train_vec = normalize(X_train_vec, norm='l2')\n",
    "X_val_vec = normalize(X_val_vec, norm='l2')\n",
    "X_test_vec = normalize(X_test_vec, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2949f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Обучаем SVM...\n",
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n",
      "\n",
      " Лучшие параметры для SVM: {'cls__C': 0.01, 'cls__class_weight': 'balanced', 'cls__max_iter': 30000}\n",
      " Лучший F1-macro (train CV): 0.7402\n",
      "\n",
      " Валидация (SVM):\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment       0.83      0.56      0.67         9\n",
      "         geography       0.46      0.75      0.57         8\n",
      "            health       0.64      0.82      0.72        11\n",
      "          politics       0.90      0.64      0.75        14\n",
      "science/technology       0.71      0.80      0.75        25\n",
      "            sports       0.71      0.83      0.77        12\n",
      "            travel       0.71      0.50      0.59        20\n",
      "\n",
      "          accuracy                           0.70        99\n",
      "         macro avg       0.71      0.70      0.69        99\n",
      "      weighted avg       0.72      0.70      0.70        99\n",
      "\n",
      "\n",
      " Лучшая модель: SVM\n",
      "\n",
      " Тест:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment       0.60      0.47      0.53        19\n",
      "         geography       0.64      0.94      0.76        17\n",
      "            health       0.65      0.77      0.71        22\n",
      "          politics       1.00      0.80      0.89        30\n",
      "science/technology       0.77      0.80      0.79        51\n",
      "            sports       0.92      0.88      0.90        25\n",
      "            travel       0.78      0.72      0.75        40\n",
      "\n",
      "          accuracy                           0.77       204\n",
      "         macro avg       0.77      0.77      0.76       204\n",
      "      weighted avg       0.79      0.77      0.77       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = {\n",
    "    'SVM': (LinearSVC(random_state=42),\n",
    "            {\n",
    "                'cls__C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n",
    "                'cls__class_weight': ['balanced'],\n",
    "                'cls__max_iter': [30000, 10000, 15000, 20000, 100000]\n",
    "            }),\n",
    "    \n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    print(f\"\\n Обучаем {name}...\")\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()), # Нормализует признаки(усредняет и масштабирует до стандартного отклонения 1)\n",
    "        ('cls', model)\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=params,\n",
    "        scoring='f1_macro',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train_vec, train_data[1])\n",
    "    best_models[name] = grid.best_estimator_\n",
    "\n",
    "    print(f\"\\n Лучшие параметры для {name}: {grid.best_params_}\")\n",
    "    print(f\" Лучший F1-macro (train CV): {grid.best_score_:.4f}\")\n",
    "\n",
    "    # Валидация\n",
    "    print(f\"\\n Валидация ({name}):\")\n",
    "    y_val_pred = grid.predict(X_val_vec)\n",
    "    print(classification_report(val_data[1], y_val_pred, target_names=classes_list))\n",
    "\n",
    "# Финальная модель и тест\n",
    "\n",
    "best_model_name = max(best_models.keys(), key=lambda n: best_models[n].score(X_val_vec, val_data[1]))\n",
    "final_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"\\n Лучшая модель: {best_model_name}\")\n",
    "\n",
    "print(\"\\n Тест:\")\n",
    "y_test_pred = final_model.predict(X_test_vec)\n",
    "print(classification_report(test_data[1], y_test_pred, target_names=classes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f91ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
